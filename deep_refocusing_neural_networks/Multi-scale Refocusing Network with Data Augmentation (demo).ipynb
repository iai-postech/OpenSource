{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from scipy import misc\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define simple but powerful data augmentation technique\n",
    "def data_augmentation(image, label):\n",
    "    cut_size = int(crop_size / 4)\n",
    "    \n",
    "    x_size = image.shape[0]\n",
    "    y_size = image.shape[1]\n",
    "    start_x = np.random.randint(x_size-cut_size)\n",
    "    start_y = np.random.randint(y_size-cut_size)\n",
    "\n",
    "    cut_label = label[start_x:start_x+cut_size, start_y:start_y+cut_size].copy()\n",
    "\n",
    "    image[start_x:start_x+cut_size, start_y:start_y+cut_size] = cut_label\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "crop_size = 256\n",
    "factor = 2\n",
    "\n",
    "# define training data loader\n",
    "current_batch = 0\n",
    "def train_batch_maker(batch_size):\n",
    "    global current_batch\n",
    "    global train_defocus_files\n",
    "    global train_infocus_files\n",
    "\n",
    "    if len(train_defocus_files) - current_batch >= batch_size:\n",
    "        batch_train_defocus_files = train_defocus_files[current_batch:current_batch+batch_size]\n",
    "        batch_train_infocus_files = train_infocus_files[current_batch:current_batch+batch_size]\n",
    "        current_batch += batch_size\n",
    "    else :\n",
    "        idx_train = np.arange(len(train_defocus_files))\n",
    "        np.random.shuffle(idx_train)\n",
    "        batch_train_defocus_files = train_defocus_files[idx_train]\n",
    "        batch_train_infocus_files = train_infocus_files[idx_train]\n",
    "\n",
    "        current_batch = 0\n",
    "        batch_train_defocus_files = train_defocus_files[current_batch:current_batch+batch_size]\n",
    "        batch_train_infocus_files = train_infocus_files[current_batch:current_batch+batch_size]\n",
    "\n",
    "    train_defocus_coarsest = []\n",
    "    train_defocus_intermediate = []\n",
    "    train_defocus_finer = []\n",
    "\n",
    "    train_infocus_coarsest = []\n",
    "    train_infocus_intermediate = []\n",
    "    train_infocus_finer = []\n",
    "\n",
    "    for image, label in zip(batch_train_defocus_files, batch_train_infocus_files):\n",
    "        temp_image = Image.open(image)\n",
    "        temp_image = np.array(temp_image)\n",
    "        temp_label = Image.open(label)\n",
    "        temp_label = np.array(temp_label)\n",
    "\n",
    "        x_size = temp_image.shape[0]\n",
    "        y_size = temp_image.shape[1]\n",
    "        start_x = np.random.randint(x_size-crop_size)\n",
    "        start_y = np.random.randint(y_size-crop_size)\n",
    "\n",
    "        temp_image = temp_image[start_x:start_x+crop_size, start_y:start_y+crop_size]\n",
    "        temp_label = temp_label[start_x:start_x+crop_size, start_y:start_y+crop_size]\n",
    "\n",
    "        temp_image, temp_label = data_augmentation(temp_image, temp_label)\n",
    "        \n",
    "        temp_image_finer = temp_image.copy()[:,:,np.newaxis]\n",
    "        temp_image_intermediate = misc.imresize(temp_image, 1.0/factor, interp = 'bicubic')[:,:,np.newaxis]\n",
    "        temp_image_coarsest = misc.imresize(temp_image, 1.0/(factor**2), interp = 'bicubic')[:,:,np.newaxis]\n",
    "\n",
    "        temp_label_finer = temp_label.copy()[:,:,np.newaxis]\n",
    "        temp_label_intermediate = misc.imresize(temp_label, 1.0/factor, interp = 'bicubic')[:,:,np.newaxis]\n",
    "        temp_label_coarsest = misc.imresize(temp_label, 1.0/(factor**2), interp = 'bicubic')[:,:,np.newaxis]\n",
    "\n",
    "        train_defocus_coarsest.append(temp_image_coarsest / 255.0)\n",
    "        train_defocus_intermediate.append(temp_image_intermediate / 255.0)\n",
    "        train_defocus_finer.append(temp_image_finer / 255.0)\n",
    "\n",
    "        train_infocus_coarsest.append(temp_label_coarsest / 255.0)\n",
    "        train_infocus_intermediate.append(temp_label_intermediate / 255.0)\n",
    "        train_infocus_finer.append(temp_label_finer / 255.0)\n",
    "\n",
    "    train_defocus_coarsest = np.array(train_defocus_coarsest)\n",
    "    train_defocus_intermediate = np.array(train_defocus_intermediate)\n",
    "    train_defocus_finer = np.array(train_defocus_finer)\n",
    "\n",
    "    train_infocus_coarsest = np.array(train_infocus_coarsest)\n",
    "    train_infocus_intermediate = np.array(train_infocus_intermediate)\n",
    "    train_infocus_finer = np.array(train_infocus_finer)\n",
    "    \n",
    "    return train_defocus_coarsest, train_defocus_intermediate, train_defocus_finer, train_infocus_coarsest, train_infocus_intermediate, train_infocus_finer \n",
    "\n",
    "# define validation data loader\n",
    "def valid_batch_maker(valid_defocus_files, valid_infocus_files):\n",
    "    valid_defocus_coarsest = []\n",
    "    valid_defocus_intermediate = []\n",
    "    valid_defocus_finer = []\n",
    "    \n",
    "    valid_infocus_coarsest = []\n",
    "    valid_infocus_intermediate = []\n",
    "    valid_infocus_finer = []\n",
    "    \n",
    "    for image, label in zip(valid_defocus_files, valid_infocus_files):\n",
    "        temp_image = Image.open(image)\n",
    "        temp_image = np.array(temp_image)\n",
    "        temp_label = Image.open(label)\n",
    "        temp_label = np.array(temp_label)\n",
    "\n",
    "        start_x = 250\n",
    "        start_y = 250\n",
    "\n",
    "        temp_image = temp_image[start_x:start_x+crop_size, start_y:start_y+crop_size]\n",
    "        temp_label = temp_label[start_x:start_x+crop_size, start_y:start_y+crop_size]\n",
    "\n",
    "        temp_image, temp_label = data_augmentation(temp_image, temp_label)\n",
    "\n",
    "        temp_image_finer = temp_image.copy()[:,:,np.newaxis]\n",
    "        temp_image_intermediate = misc.imresize(temp_image, 1.0/factor, interp = 'bicubic')[:,:,np.newaxis]\n",
    "        temp_image_coarsest = misc.imresize(temp_image, 1.0/(factor**2), interp = 'bicubic')[:,:,np.newaxis]\n",
    "\n",
    "        temp_label_finer = temp_label.copy()[:,:,np.newaxis]\n",
    "        temp_label_intermediate = misc.imresize(temp_label, 1.0/factor, interp = 'bicubic')[:,:,np.newaxis]\n",
    "        temp_label_coarsest = misc.imresize(temp_label, 1.0/(factor**2), interp = 'bicubic')[:,:,np.newaxis]\n",
    "\n",
    "        valid_defocus_coarsest.append(temp_image_coarsest / 255.0)\n",
    "        valid_defocus_intermediate.append(temp_image_intermediate / 255.0)\n",
    "        valid_defocus_finer.append(temp_image_finer / 255.0)\n",
    "\n",
    "        valid_infocus_coarsest.append(temp_label_coarsest / 255.0)\n",
    "        valid_infocus_intermediate.append(temp_label_intermediate / 255.0)\n",
    "        valid_infocus_finer.append(temp_label_finer / 255.0)\n",
    "\n",
    "    valid_defocus_coarsest = np.array(valid_defocus_coarsest)\n",
    "    valid_defocus_intermediate = np.array(valid_defocus_intermediate)\n",
    "    valid_defocus_finer = np.array(valid_defocus_finer)\n",
    "\n",
    "    valid_infocus_coarsest = np.array(valid_infocus_coarsest)\n",
    "    valid_infocus_intermediate = np.array(valid_infocus_intermediate)\n",
    "    valid_infocus_finer = np.array(valid_infocus_finer)\n",
    "    \n",
    "    return valid_defocus_coarsest, valid_defocus_intermediate, valid_defocus_finer, valid_infocus_coarsest, valid_infocus_intermediate, valid_infocus_finer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## glob your dataset for your own MRN\n",
    "# train_defocus_files = np.sort(np.array(glob())\n",
    "# train_infocus_files = np.sort(np.array(glob())\n",
    "# valid_defocus_files = np.sort(np.array(glob())\n",
    "# valid_infocus_files = np.sort(np.array(glob())\n",
    "\n",
    "test_defocus_files = glob('./data_files/*.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Multi-scale Refocusing Network (MRN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Build MRN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coarsest = tf.placeholder(tf.float32, [None, None, None, 1], name='x_coarsest')\n",
    "x_intermediate = tf.placeholder(tf.float32, [None, None, None, 1], name='x_intermediate')\n",
    "x_finer = tf.placeholder(tf.float32, [None, None, None, 1], name='x_finer')\n",
    "\n",
    "y_coarsest = tf.placeholder(tf.float32, [None, None, None, 1], name='y_coarsest')\n",
    "y_intermediate = tf.placeholder(tf.float32, [None, None, None, 1], name='y_intermediate')\n",
    "y_finer = tf.placeholder(tf.float32, [None, None, None, 1], name='y_finer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResidualBlock(x, kernel_size, filters, strides = 1):\n",
    "    skip = x\n",
    "    x = tf.layers.conv2d(x, \n",
    "                         kernel_size = kernel_size,\n",
    "                         filters = filters,\n",
    "                         strides = strides,\n",
    "                         padding = 'same',\n",
    "                         use_bias = False)\n",
    "    x = tf.contrib.keras.layers.PReLU(shared_axes = [1,2])(x)\n",
    "    x = tf.layers.conv2d(x,\n",
    "                         kernel_size = kernel_size,\n",
    "                         filters = filters,\n",
    "                         strides = strides,\n",
    "                         padding = 'same',\n",
    "                         use_bias = False)\n",
    "    x = x + skip\n",
    "    return x\n",
    "\n",
    "def Upsample2xBlock(x, kernel_size, filters, name, strides = 1):\n",
    "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             kernel_size = kernel_size,\n",
    "                             filters = filters,\n",
    "                             strides = strides,\n",
    "                             padding = 'same')\n",
    "        x = tf.depth_to_space(x, 2)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_coarsest(x, num_blocks):\n",
    "    with tf.variable_scope('resnet_coarsest', reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             kernel_size = 5,\n",
    "                             filters = 64,\n",
    "                             strides = 1,\n",
    "                             padding = 'same')\n",
    "        x = tf.contrib.keras.layers.PReLU(shared_axes = [1,2])(x)\n",
    "        skip = x\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            x = ResidualBlock(x, kernel_size = 5, filters = 64, strides = 1)\n",
    "            \n",
    "        x = tf.layers.conv2d(x,\n",
    "                             kernel_size = 5,\n",
    "                             filters = 64,\n",
    "                             strides = 1,\n",
    "                             padding = 'same',\n",
    "                             use_bias = False)\n",
    "        x = x + skip\n",
    "        \n",
    "        x = tf.layers.conv2d(x,\n",
    "                             kernel_size = 5,\n",
    "                             filters = 1,\n",
    "                             strides = 1,\n",
    "                             padding = 'same',\n",
    "                             name = 'forward')\n",
    "        return tf.nn.sigmoid(x)\n",
    "\n",
    "def resnet_intermediate(x, num_blocks):\n",
    "    with tf.variable_scope('resnet_intermediate', reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             kernel_size = 5,\n",
    "                             filters = 64,\n",
    "                             strides = 1,\n",
    "                             padding = 'same')\n",
    "        x = tf.contrib.keras.layers.PReLU(shared_axes = [1,2])(x)\n",
    "        skip = x\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            x = ResidualBlock(x, kernel_size = 5, filters = 64, strides = 1)\n",
    "            \n",
    "        x = tf.layers.conv2d(x,\n",
    "                             kernel_size = 5,\n",
    "                             filters = 64,\n",
    "                             strides = 1,\n",
    "                             padding = 'same',\n",
    "                             use_bias = False)\n",
    "        x = x + skip\n",
    "        \n",
    "        x = tf.layers.conv2d(x,\n",
    "                             kernel_size = 5,\n",
    "                             filters = 1,\n",
    "                             strides = 1,\n",
    "                             padding = 'same',\n",
    "                             name = 'forward')\n",
    "        return tf.nn.sigmoid(x)\n",
    "    \n",
    "def resnet_finer(x, num_blocks):\n",
    "    with tf.variable_scope('resnet_finer', reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.layers.conv2d(x,\n",
    "                             kernel_size = 5,\n",
    "                             filters = 64,\n",
    "                             strides = 1,\n",
    "                             padding = 'same')\n",
    "        x = tf.contrib.keras.layers.PReLU(shared_axes = [1,2])(x)\n",
    "        skip = x\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            x = ResidualBlock(x, kernel_size = 5, filters = 64, strides = 1)\n",
    "            \n",
    "        x = tf.layers.conv2d(x,\n",
    "                             kernel_size = 5,\n",
    "                             filters = 64,\n",
    "                             strides = 1,\n",
    "                             padding = 'same',\n",
    "                             use_bias = False)\n",
    "        x = x + skip\n",
    "        \n",
    "        x = tf.layers.conv2d(x,\n",
    "                             kernel_size = 5,\n",
    "                             filters = 1,\n",
    "                             strides = 1,\n",
    "                             padding = 'same',\n",
    "                             name = 'forward')\n",
    "        return tf.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coarsest scale subnetwork\n",
    "refocus_coarsest = resnet_coarsest(x_coarsest, 16)\n",
    "refocus_coarsest_upconv = Upsample2xBlock(refocus_coarsest, kernel_size = 3, filters = 4, name = 'upconv_for_intermediate')\n",
    "refocus_coarsest_upconv_concat = tf.concat((refocus_coarsest_upconv, x_intermediate), axis = 3)\n",
    "\n",
    "# intermediate scale subnetwork\n",
    "refocus_intermediate = resnet_intermediate(refocus_coarsest_upconv_concat, 16)\n",
    "refocus_intermediate_upconv = Upsample2xBlock(refocus_intermediate, kernel_size = 3, filters = 4, name = 'upconv_for_finer')\n",
    "refocus_intermediate_upconv_concat = tf.concat((refocus_intermediate_upconv, x_finer), axis = 3)\n",
    "\n",
    "# finer scale subnetwork\n",
    "refocus_finer = resnet_finer(refocus_intermediate_upconv_concat, 16)\n",
    "\n",
    "# loss function\n",
    "loss_coarsest = tf.reduce_mean(tf.abs(y_coarsest - refocus_coarsest))\n",
    "loss_intermediate = tf.reduce_mean(tf.abs(y_intermediate - refocus_intermediate))\n",
    "loss_finer = tf.reduce_mean(tf.abs(y_finer - refocus_finer))\n",
    "\n",
    "# learning rate\n",
    "LR = 0.00005\n",
    "global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "learning_rate = tf.train.exponential_decay(LR, global_step, 50000, 0.1, staircase = False)\n",
    "incr_global_step = tf.assign(global_step, global_step + 1)\n",
    "\n",
    "# variable list\n",
    "var_coarsest = [var for var in tf.get_collection('trainable_variables') if 'resnet_coarsest' in var.name]\n",
    "var_intermediate = [var for var in tf.get_collection('trainable_variables') if 'resnet_intermediate' in var.name or 'upconv_for_intermediate' in var.name]\n",
    "var_finer = [var for var in tf.get_collection('trainable_variables') if 'resnet_finer' in var.name or 'upconv_for_finer' in var.name]\n",
    "\n",
    "# optimizer\n",
    "optm_coarsest = tf.train.AdamOptimizer(learning_rate).minimize(loss_coarsest, var_list = var_coarsest)\n",
    "optm_intermediate = tf.train.AdamOptimizer(learning_rate).minimize(loss_intermediate, var_list = var_intermediate)\n",
    "optm_finer = tf.train.AdamOptimizer(learning_rate).minimize(loss_finer, var_list = var_finer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # define training parameters\n",
    "# n_iter = 50000\n",
    "# n_prt = 100\n",
    "# n_batch = 5\n",
    "# save_criteria = 10\n",
    "\n",
    "# # open tf session\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# saver = tf.train.Saver()\n",
    "\n",
    "# # load validation dataset\n",
    "# valid_defocus_coarsest, valid_defocus_intermediate, valid_defocus_finer, valid_infocus_coarsest, valid_infocus_intermediate, valid_infocus_finer  = valid_batch_maker(valid_defocus_files, valid_infocus_files)\n",
    "\n",
    "# # trainining phase\n",
    "# for epoch in range(n_iter):\n",
    "    \n",
    "#     # load batch training dataset\n",
    "#     train_defocus_coarsest, train_defocus_intermediate, train_defocus_finer, train_infocus_coarsest, train_infocus_intermediate, train_infocus_finer = train_batch_maker(n_batch)\n",
    "    \n",
    "#     # optimize MRN\n",
    "#     sess.run([optm_coarsest, optm_intermediate, optm_finer], feed_dict = {x_coarsest: train_defocus_coarsest, \n",
    "#                                                                           x_intermediate: train_defocus_intermediate,\n",
    "#                                                                           x_finer: train_defocus_finer, \n",
    "#                                                                           y_coarsest: train_infocus_coarsest, \n",
    "#                                                                           y_intermediate: train_infocus_intermediate, \n",
    "#                                                                           y_finer: train_infocus_finer})\n",
    "#     sess.run(incr_global_step)\n",
    "    \n",
    "#     # save a best model\n",
    "#     criteria_temp = sess.run(loss_finer, feed_dict = {x_coarsest: valid_defocus_coarsest, \n",
    "#                                                       x_intermediate: valid_defocus_intermediate,\n",
    "#                                                       x_finer: valid_defocus_finer, \n",
    "#                                                       y_coarsest: valid_infocus_coarsest, \n",
    "#                                                       y_intermediate: valid_infocus_intermediate, \n",
    "#                                                       y_finer: valid_infocus_finer})\n",
    "#     if save_criteria > criteria_temp:\n",
    "#         save_criteria = criteria_temp\n",
    "#         saver.save(sess, './model/MRN.ckpt')\n",
    "    \n",
    "#     # record loss graphs, and print refocused SEM image \n",
    "#     if epoch % n_prt == 0:        \n",
    "        \n",
    "#         print('Epoch:', '%04d' % epoch)\n",
    "#         refocus_img = sess.run(refocus_finer, feed_dict = {x_coarsest: train_defocus_coarsest[:1], \n",
    "#                                                            x_intermediate: train_defocus_intermediate[:1],\n",
    "#                                                            x_finer: train_defocus_finer[:1]})\n",
    "        \n",
    "#         plt.figure(figsize = (5,5))\n",
    "#         plt.imshow(refocus_img[0,:,:,0], cmap = 'gray')\n",
    "#         plt.axis('off')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the best model\n",
    "save_file = './model/MRN.ckpt'\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "saver.restore(sess, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# refocus SEM images\n",
    "for defocus_file in test_defocus_files:\n",
    "    print(defocus_file)\n",
    "    \n",
    "    defocus_image = Image.open(defocus_file)\n",
    "    defocus_image = np.array(defocus_image)\n",
    "    \n",
    "    if (defocus_image.shape[0] / factor**2) % 1 != 0 or (defocus_image.shape[1] / factor**2) % 1 != 0:\n",
    "        new_x_shape = int(defocus_image.shape[0] / factor**2) * factor**2\n",
    "        new_y_shape = int(defocus_image.shape[1] / factor**2) * factor**2\n",
    "        defocus_image = defocus_image[:new_x_shape,:new_y_shape]\n",
    "\n",
    "    defocus_image_finer = defocus_image.copy()[np.newaxis,:,:,np.newaxis] / 255\n",
    "    defocus_image_intermediate = misc.imresize(defocus_image, 1.0/factor, interp = 'bicubic')[np.newaxis,:,:,np.newaxis] / 255\n",
    "    defocus_image_coarsest = misc.imresize(defocus_image, 1.0/(factor**2), interp = 'bicubic')[np.newaxis,:,:,np.newaxis] / 255\n",
    "\n",
    "    refocus_img = sess.run(refocus_finer, feed_dict = {x_coarsest: defocus_image_coarsest, \n",
    "                                                     x_intermediate: defocus_image_intermediate,\n",
    "                                                     x_finer: defocus_image_finer})\n",
    "    \n",
    "    plt.figure(figsize = (20,20))\n",
    "    plt.imshow(refocus_img[0,:,:,0], cmap = 'gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (20,20))\n",
    "    plt.imshow(defocus_image[:,:], cmap = 'gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
